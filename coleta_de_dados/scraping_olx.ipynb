{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "from functools import reduce\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import string\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BackupApiResp:\n",
    "    def __init__(self,path_file_bk, max_days_to_valid_data = 720): \n",
    "        self.__con = sqlite3.connect(path_file_bk)\n",
    "        self.__con.row_factory = sqlite3.Row\n",
    "        self.days_to_expires = max_days_to_valid_data #validade dos dados\n",
    "        self.today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        self.__init_db()\n",
    "    \n",
    "    def __treat_text_dict_to_sql(text_dict):\n",
    "        for key in text_dict:\n",
    "            text_dict[key] = text_dict[key].replace(\"'\",\"''\")\n",
    "        return text_dict\n",
    "    def __init_db(self):\n",
    "        sql_table_def = (\n",
    "            'CREATE TABLE IF NOT EXISTS ' \n",
    "            'anuncios_resumo ('\n",
    "                'url_lista TEXT,'\n",
    "                'numero_pagina_lista INTEGER,'\n",
    "                'url_anuncio TEXT,'\n",
    "                'categoria_completa TEXT,'\n",
    "                'categoria_atual TEXT,'\n",
    "                'titulo_anuncio TEXT,'\n",
    "                'detalhes_anuncio TEXT,'\n",
    "                'preco_anuncio FLOAT,'\n",
    "                'url_img_principal TEXT,'\n",
    "                'qt_img_anuncio INTEGER,'\n",
    "                'localizacao_completa TEXT,'\n",
    "                'localizacao_complemento TEXT,'\n",
    "                'anuncio_profissional INTEGER,'\n",
    "                'data_publicacao_anuncio DATETIME,'\n",
    "                'data_coleta_dados DATETIME'\n",
    "            ');'\n",
    "        )\n",
    "\n",
    "        sql_index1_def = (\n",
    "            'CREATE INDEX IF NOT EXISTS '\n",
    "            'index_url_anuncio_ar ON '\n",
    "            'anuncios_resumo ('\n",
    "                'url_anuncio ASC'\n",
    "            ');'\n",
    "        )\n",
    "\n",
    "        sql_index2_def = (\n",
    "            'CREATE INDEX IF NOT EXISTS '\n",
    "            'index_url_lista_ar ON '\n",
    "            'anuncios_resumo ('\n",
    "                'url_lista ASC'\n",
    "            ');'\n",
    "        )\n",
    "        sql_index_control = (\n",
    "            'CREATE INDEX IF NOT EXISTS '\n",
    "            'index_table_control ON '\n",
    "            'scraping_control ('\n",
    "                'url_base ASC'\n",
    "            ');'\n",
    "        )\n",
    "        sql_table_control = (\n",
    "            'CREATE TABLE IF NOT EXISTS ' \n",
    "            'scraping_control ('\n",
    "                'url_base TEXT,'\n",
    "                'qt_last_scraping INTEGER,'\n",
    "                'id_session TEXT,'\n",
    "                'date_init_scraping DATETIME,'\n",
    "                'date_end_scraping DATETIME'\n",
    "            ');'\n",
    "        )\n",
    "        self.__con.execute(sql_table_def)\n",
    "        self.__con.execute(sql_index1_def)\n",
    "        self.__con.execute(sql_index2_def)\n",
    "        self.__con.execute(sql_table_control)\n",
    "        self.__con.execute(sql_index_control)\n",
    "        self.__con.commit()\n",
    "\n",
    "    def add_rows_anuncios_resumo(self,rows):\n",
    "        if isinstance(rows,list):\n",
    "            if len(rows) > 0 and isinstance(rows[0],dict):\n",
    "                sql_insert_data = \"\"\"INSERT INTO anuncios_resumo (\n",
    "                                                    url_lista,\n",
    "                                                    numero_pagina_lista,\n",
    "                                                    url_anuncio,\n",
    "                                                    categoria_completa,\n",
    "                                                    categoria_atual,\n",
    "                                                    titulo_anuncio,\n",
    "                                                    detalhes_anuncio,\n",
    "                                                    preco_anuncio,\n",
    "                                                    url_img_principal,\n",
    "                                                    qt_img_anuncio,\n",
    "                                                    localizacao_completa,\n",
    "                                                    localizacao_complemento,\n",
    "                                                    anuncio_profissional,\n",
    "                                                    data_publicacao_anuncio,\n",
    "                                                    data_coleta_dados ) VALUES\"\"\"\n",
    "                sql_rows = []\n",
    "                for row in rows:\n",
    "                    row = BackupApiResp.__treat_text_dict_to_sql(row)\n",
    "                    sql_rows.append(f\"\"\"('{row.get('url_lista',\"erro\")}',\n",
    "                                          {row.get('numero_pagina_lista',\"-1\")},\n",
    "                                         '{row.get('url_anuncio',\"erro\")}',\n",
    "                                         '{row.get('categoria_completa',\"erro\")}',\n",
    "                                         '{row.get('categoria_atual',\"erro\")}',\n",
    "                                         '{row.get('titulo_anuncio',\"erro\")}',\n",
    "                                         '{row.get('detalhes_anuncio',\"erro\")}',\n",
    "                                          {row.get('preco_anuncio',\"-1\")},\n",
    "                                         '{row.get('url_img_principal',\"erro\")}',\n",
    "                                          {row.get('qt_img_anuncio',\"-1\")},\n",
    "                                         '{row.get('localizacao_completa',\"erro\")}',\n",
    "                                         '{row.get('localizacao_complemento',\"erro\")}',\n",
    "                                          {row.get('anuncio_profissional',\"-1\")},\n",
    "                                         '{row.get('data_publicacao_anuncio',self.today)}',\n",
    "                                         '{self.today}')\"\"\")\n",
    "                sql_insert_data += ','.join(sql_rows) + ';'\n",
    "                # print(sql_insert_data)\n",
    "                self.__con.execute(sql_insert_data)\n",
    "                self.__con.commit()\n",
    "    \n",
    "    def add_url_scraping_control(self,url_base):\n",
    "        sql_insert_data = (\n",
    "            \"INSERT INTO scraping_control (url_base) \"\n",
    "            \"VALUES (\"\n",
    "                f\"'{url_base}'\"\n",
    "            \");\"\n",
    "        )\n",
    "        self.__con.execute(sql_insert_data)\n",
    "        self.__con.commit()\n",
    "    \n",
    "    def init_scraping_url_base(self,url_base,id_session='-'):\n",
    "        sql_insert_data = f\"\"\"UPDATE scraping_control\n",
    "                                SET date_init_scraping = '{datetime.now().strftime(\"%Y-%m-%d %H:%M\")}',\n",
    "                                    id_session = '{id_session}',\n",
    "                                    date_end_scraping = NULL\n",
    "                              WHERE url_base = '{url_base}' AND \n",
    "                              ((id_session is NULL) OR \n",
    "                              COALESCE(julianday('now') - julianday(date_init_scraping),99)>1)\n",
    "                              \"\"\"\n",
    "     \n",
    "        self.__con.execute(sql_insert_data)\n",
    "        self.__con.commit()\n",
    "    \n",
    "    def verify_id_session(self,url_base,id_session): #util if use a cloud db\n",
    "        sql =  f\"SELECT url_base FROM scraping_control where url_base = '{url_base}' and id_session = '{id_session}'\"\n",
    "        result = self.__con.execute(sql).fetchone()\n",
    "        return True if result else False  \n",
    "    \n",
    "    def get_urls_base_in_db(self,only_not_scraped = False):\n",
    "        sql_data = f\"\"\"SELECT url_base FROM scraping_control\"\"\"\n",
    "        sql_data += f\"\"\" where date_end_scraping is null order by id_session, date_init_scraping desc \"\"\" if only_not_scraped else \" order by id_session, qt_last_scraping desc\"\n",
    "     \n",
    "        result = self.__con.execute(sql_data).fetchall()\n",
    "        return [row['url_base'] for row in result]\n",
    "        \n",
    "    def end_scraping_url_base(self,url_base,qt_last_scraping):\n",
    "        sql_insert_data = f\"\"\"UPDATE scraping_control\n",
    "                                SET date_end_scraping = '{datetime.now().strftime(\"%Y-%m-%d %H:%M\")}',\n",
    "                                    qt_last_scraping = {qt_last_scraping},\n",
    "                                    id_session = NULL\n",
    "                              WHERE url_base = '{url_base}' \"\"\"\n",
    "     \n",
    "        self.__con.execute(sql_insert_data)\n",
    "        self.__con.commit()\n",
    "         \n",
    "    def date_is_valid(self,date:str):\n",
    "        data_age_days = (datetime.now()-datetime.fromisoformat(date)).days\n",
    "        return (data_age_days <= self.days_to_expires)\n",
    "\n",
    "    def get_all_urls_ads(self,url_lista):\n",
    "        sql_data = f\"\"\"SELECT url_anuncio FROM anuncios_resumo where url_lista = '{url_lista}'\"\"\"\n",
    "        result = self.__con.execute(sql_data).fetchall()\n",
    "        return [row['url_anuncio'] for row in result]\n",
    "\n",
    "    def has_url_ad_in_anuncio_resumo(self,url_ad):\n",
    "        sql = f\"\"\" select url_anuncio from anuncios_resumo\n",
    "                    where url_anuncio = '{url_ad}'\n",
    "                    limit 1 \"\"\"\n",
    "\n",
    "        result = self.__con.execute(sql).fetchone()\n",
    "        return True if result else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_verbose_function(verbose):\n",
    "    if verbose:\n",
    "        return print\n",
    "    else:\n",
    "        return lambda x: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_olx_to_datetime_str(texto_data):\n",
    "    MONTHS = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4,  'mai': 5,  'jun': 6,\n",
    "          'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}\n",
    "    data, hora = texto_data.lower().split(',')\n",
    "    data_padrao = \"\"\n",
    "    \n",
    "    if 'ontem' == data :\n",
    "        data_padrao = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    elif 'hoje' == data :\n",
    "        data_padrao = datetime.now().strftime('%Y-%m-%d')\n",
    "    else:    \n",
    "        dataP = data.split(' ')\n",
    "        dia_mes = int(dataP[0][0:2])\n",
    "        mes = MONTHS[dataP[1]]\n",
    "        ano_atual = int(datetime.now().strftime('%Y'))\n",
    "        mes_atual = int(datetime.now().strftime('%m'))\n",
    "        ano = ano_atual if mes_atual >= mes else ano_atual - 1\n",
    "        try:\n",
    "            data_padrao = datetime(year=ano, month=mes, day=dia_mes).strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            data_padrao = datetime(year=2000, month=1, day=1).strftime('%Y-%m-%d')\n",
    "  \n",
    "    \n",
    "    return data_padrao + hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_error = '29 fev, 00:03' #https://pi.olx.com.br/regiao-de-teresina-e-parnaiba/imoveis/casa-com-otima-localizacao-na-zona-leste-722939905\n",
    "# convert_date_olx_to_datetime_str(data_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Teste\n",
    "# backup = BackupApiResp(\"./banco_backup.db\")\n",
    "# # backup.has_url_ad_in_anuncio_resumo('www.teste2')\n",
    "# # backup.add_url_scraping_control('www.teste20')\n",
    "# # backup.init_scraping_url_base('www.teste20')\n",
    "# # # backup.end_scraping_url_base('www.teste11')\n",
    "# # teste = backup.get_urls_in_db()\n",
    "# backup.add_rows_anuncios_resumo([\n",
    "# {\n",
    "#     'url_lista':'url_lista',\n",
    "#     'numero_pagina_lista':'1',\n",
    "#     'url_anuncio':'url_anuncio',\n",
    "#     'categoria_completa':'categoria_completa',\n",
    "#     'categoria_atual':'categoria_atual',\n",
    "#     'titulo_anuncio':'titulo_anuncio',\n",
    "#     'detalhes_anuncio':'detalhes_anuncio',\n",
    "#     'preco_anuncio':'999',\n",
    "#     'url_img_principal':'url_img_principal',\n",
    "#     'qt_img_anuncio':'1',\n",
    "#     'localizacao_completa':'localizacao_completa',\n",
    "#     'localizacao_complemento':'localizacao_complemento',\n",
    "#     'anuncio_profissional':'-1',\n",
    "#     'data_publicacao_anuncio':'1990-01-01',\n",
    "#     'data_coleta_dados':'1990-01-01'\n",
    "# },\n",
    "# {\n",
    "#     'dadosAltearorios':111\n",
    "# }\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_olx =  { \n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "                \"origin\": \"https://olx.com.br/\",\n",
    "                \"referer\": \"https://olx.com.br/\",\n",
    "                \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"96\\\", \\\"Google Chrome\\\";v=\\\"96\\\"\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "                \"sec-fetch-dest\": \"empty\",\n",
    "                \"sec-fetch-mode\": \"cors\",\n",
    "                \"sec-fetch-site\": \"same-site\",\n",
    "                \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retroactive_links_in_div(div_class,url,verbose=True,only_endpoints=False):\n",
    "    \"\"\"\n",
    "    this code is used to get the retroactive links in the div class\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    all_url_visited = []\n",
    "    end_links = []\n",
    "    error_links = []\n",
    "    iprint = print_verbose_function(verbose)\n",
    "\n",
    "    def get_links_in_current_div(url_base,suffix='',qt_try=1):\n",
    "        nonlocal session\n",
    "        if url_base not in all_url_visited:\n",
    "            all_url_visited.append(url_base)   \n",
    "        iprint(\"*****url_base: \"+ url_base)\n",
    "        \n",
    "        try:\n",
    "            resp = session.get(url_base+suffix, headers=headers_olx, timeout=15)\n",
    "            bsObj = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            items = bsObj.find(\"div\", {\"class\":div_class})\n",
    "            qt_reg_adds = 0\n",
    "            if items:\n",
    "                links_in_div = [*map(lambda x: x['href'], items.find_all(\"a\", href=True))]\n",
    "                \n",
    "                for link in links_in_div:\n",
    "                    if link not in all_url_visited:\n",
    "                        iprint('+Redirecionamento:'+url_base+'->' + link)\n",
    "                        get_links_in_current_div(link,'/')\n",
    "                        qt_reg_adds+=1\n",
    "                \n",
    "            else:\n",
    "                print('warning-no-class:' + url_base)\n",
    "            \n",
    "            if qt_reg_adds == 0:\n",
    "                if url_base not in end_links:\n",
    "                    iprint('----end_link:'+url_base) \n",
    "                    end_links.append(url_base)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+url_base)\n",
    "            print(str(e))\n",
    "            print('!!! Restart session and Wait 10 seconds\\n\\n')\n",
    "            time.sleep(10)\n",
    "            session = requests.Session()\n",
    "            if qt_try < 6:\n",
    "                get_links_in_current_div(url_base,suffix,qt_try+1)\n",
    "            else:\n",
    "                print('!!! Error, qt attempts:'+str(qt_try)+' in:'+url_base)\n",
    "                error_links.append(url_base)\n",
    "\n",
    "    get_links_in_current_div(url)       \n",
    "    \n",
    "    return all_url_visited if only_endpoints == False else end_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test_endlink = get_retroactive_links_in_div('sc-1ncgzjx-0','https://mg.olx.com.br/belo-horizonte-e-regiao/imoveis/venda',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_category_urls():\n",
    "    \"\"\"\n",
    "    Return only the most specific category links \n",
    "    *(general categories are not returned and yours advertisements are included in specific categories)\n",
    "    * is necessary get specific categories because the general categories are limited in 5k advertisements per location\n",
    "    \"\"\"\n",
    "    url_base = \"https://www.olx.com.br/brasil\"\n",
    "    class_category_menu = \"jx24x3-2\"\n",
    "\n",
    "    all_url_visited = get_retroactive_links_in_div(class_category_menu,url_base) # end_links = False -> because there are siblings links in categories menu\n",
    "\n",
    "    most_specific_category = []\n",
    "\n",
    "    # Is necessary threat all links using Regex to get the most specific category\n",
    "    for link_ref in all_url_visited:\n",
    "        add = True\n",
    "        for link_comp in all_url_visited:\n",
    "            if re.match(link_ref, link_comp) and link_ref != link_comp:\n",
    "                add = False\n",
    "                break\n",
    "        if add:\n",
    "            most_specific_category.append(link_ref)\n",
    "\n",
    "    most_specific_category.remove(url_base)       \n",
    "    \n",
    "    return  most_specific_category   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[]]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_urls_scraping_threads(urls_base,qt_threads=10,verbose=False):\n",
    "    \"\"\"\n",
    "    Return a list of urls to be scraped (inside a specific region)\n",
    "    Is necessary to use threads because the OLX website has a lot of categories and the scraping is slow\n",
    "    \"\"\"\n",
    "    def _get_retroactive_links_in_div(div_class,url,results,index):\n",
    "        #is necessay to use this function because the get_retroactive_links_in_div function is not thread safe\n",
    "        results[index] = get_retroactive_links_in_div(div_class,url,verbose,True)\n",
    "    \n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\n",
    "    start_urls = []\n",
    "    threads = [None] * qt_threads\n",
    "    results = [None] * qt_threads\n",
    "    qt_urls_base = len(urls_base)\n",
    "    print('Start get_start_urls_scraping_threads - This Function take many time to finish')\n",
    "    for index in range(0,qt_urls_base,qt_threads):\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                threads[i] = Thread(target=_get_retroactive_links_in_div, args=(class_location_menu,urls_base[index+i],results,i))\n",
    "                threads[i].start()\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                threads[i].join()\n",
    "                print(f\"Category {urls_base[index+i]} ended: {index+i+1} / {qt_urls_base}\")\n",
    "                start_urls.extend(results[i])\n",
    "        # start_urls = sum(results,start_urls)\n",
    "        threads = [None] * qt_threads\n",
    "        results = [None] * qt_threads\n",
    "\n",
    "    return [*filter(lambda x: x != None, start_urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test = get_start_urls_scraping_threads(['https://ac.olx.com.br/hobbies-e-colecoes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_urls_scraping(urls_base,vebose=False):\n",
    "    \"\"\"\n",
    "    Return a list of urls to be scraped (inside a specific region)\n",
    "    \"\"\"\n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\n",
    "    all_url_visited = []\n",
    "    for url_base in urls_base:\n",
    "      qt_url_base = len(urls_base)\n",
    "      percetil_end = len(all_url_visited)/qt_url_base*100\n",
    "      print(f'Percentual Concluido:{percetil_end:.2f}% - {len(all_url_visited)} de {qt_url_base}')\n",
    "      all_url_visited.extend(get_retroactive_links_in_div(class_location_menu,url_base,vebose,True))\n",
    "    return all_url_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category_urls = []\n",
    "if not Path('./unique_category_urls.pkl').is_file():\n",
    "    unique_category_urls = get_unique_category_urls()\n",
    "    with open('./unique_category_urls.pkl', 'wb') as file:\n",
    "        pickle.dump(unique_category_urls, file)\n",
    "else :\n",
    "    with open('./unique_category_urls.pkl', 'rb') as file:\n",
    "        unique_category_urls = pickle.load(file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls_scraping = []\n",
    "if not Path('./start_urls_scraping.pkl').is_file():\n",
    "    start_urls_scraping = get_start_urls_scraping_threads(unique_category_urls,12)\n",
    "    with open('./start_urls_scraping.pkl', 'wb') as file:\n",
    "        pickle.dump(start_urls_scraping, file) \n",
    "else :\n",
    "    with open('./start_urls_scraping.pkl', 'rb') as file:\n",
    "        start_urls_scraping = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banco_olx = BackupApiResp(\"./banco_scraping_olx.db\")\n",
    "all_urls = banco_olx.get_urls_base_in_db()\n",
    "urls_to_add_db = [*filter(lambda x: x not in all_urls, start_urls_scraping)]\n",
    "# add new urls to db for scrapping\n",
    "for url in urls_to_add_db:\n",
    "    banco_olx.add_url_scraping_control(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_olx_page(bsObj,list_url,page_number):\n",
    "    \n",
    "    div_category = bsObj.find(\"div\", {\"class\": \"otEye\"})\n",
    "    list_category = div_category.findAll(text=True) if div_category else []\n",
    "    current_category = list_category[-1] if len(list_category) > 0 else 'Error'\n",
    "    complete_category = ';'.join(list_category[1:]) if len(list_category) > 0 else 'Error'\n",
    "    div_location = bsObj.find(\"div\", {\"class\": \"sc-gPWkxV UqLlM\"})\n",
    "    complete_location = div_location.text.replace('>',';') if div_location else ''\n",
    "    ad_list = bsObj.findAll(\"li\", {\"class\": \"sc-1fcmfeb-2\"})\n",
    "    rows_to_save_db = []\n",
    "    for ad_resume in ad_list:\n",
    "\n",
    "        adv_url_tag = ad_resume.find(\"a\",href=True)\n",
    "        if adv_url_tag:\n",
    "            thumb_img_tag = adv_url_tag.find(\"img\",src=True)\n",
    "            qt_img_tag = ad_resume.find(\"span\",{\"class\":\"bYaEay\"})\n",
    "            desc_tag = ad_resume.find(\"h2\",text=True) \n",
    "            price_tag = ad_resume.find(\"span\",{\"class\":\"kHeyHD\"})\n",
    "            detail_tag = ad_resume.find(\"div\",{\"class\":\"jEDFNq\"})\n",
    "            address_detail_tag = ad_resume.find(\"span\",{\"class\":\"iDvjkv\"})\n",
    "            type_seller_tag = ad_resume.find(\"span\",{\"class\":\"jGYopB\"})\n",
    "            date_tag = ad_resume.find(\"span\",{\"class\":\"javKJU\"})\n",
    "            price = price_tag.text if price_tag else ''\n",
    "            \n",
    "            data_to_insert = {\n",
    "                                'url_lista': list_url,\n",
    "                                'numero_pagina_lista': str(page_number),\n",
    "                                'url_anuncio': adv_url_tag['href'] if adv_url_tag else '',\n",
    "                                'categoria_completa': complete_category,\n",
    "                                'categoria_atual': current_category,\n",
    "                                'titulo_anuncio': desc_tag.text if desc_tag else '',\n",
    "                                'detalhes_anuncio': detail_tag.getText() if detail_tag else '',\n",
    "                                'preco_anuncio': re.sub('[^0-9,]','',price).replace(',','.') if price else 'NULL',\n",
    "                                'url_img_principal': thumb_img_tag['src'] if thumb_img_tag else '',\n",
    "                                'qt_img_anuncio': qt_img_tag.text if qt_img_tag else '0',\n",
    "                                'localizacao_completa': complete_location,\n",
    "                                'localizacao_complemento': address_detail_tag.text if address_detail_tag else '',\n",
    "                                'anuncio_profissional':'1' if type_seller_tag else '0',\n",
    "                                'data_publicacao_anuncio': convert_date_olx_to_datetime_str(date_tag.text) if date_tag else datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "                            }\n",
    "            rows_to_save_db.append(data_to_insert)\n",
    "    \n",
    "    return rows_to_save_db\n",
    "    # banco_olx.add_rows_anuncios_resumo(rows_to_save_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_insert_data_olx(url_end_point,urls_already_access=[],page_number=1,session=None,qt_try=1,force_search=False):\n",
    "    # print(f\"Scraping {url_end_point} page {page_number}\")\n",
    "    \n",
    "    query_Parameters = {'sf':1} if page_number == 1 else {} # get data in order of publication (THIS CASE IS POSSIBLE STOP SCRAPING IF SEE ONE AD ALREADY SCRAPED)\n",
    "    _session = session if session else requests.Session()\n",
    "    # try:\n",
    "    resp = _session.get(url_end_point, params=query_Parameters, headers=headers_olx, timeout=15)\n",
    "    bsObj = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    _problem = True if bsObj.find('title',text=\"Ops!!\") else False\n",
    "    if not _problem :\n",
    "        _url_end_point = url_end_point.split('?')[0]\n",
    "        all_row_scraped = get_data_from_olx_page(bsObj,_url_end_point,page_number)\n",
    "        rows_to_save_db = [*filter(lambda x: x['url_anuncio'] not in urls_already_access, all_row_scraped)] # add only new ads\n",
    "        # banco_olx.add_rows_anuncios_resumo(rows_to_save_db)\n",
    "        \n",
    "        if len(rows_to_save_db) != len(all_row_scraped) and not force_search:  # if some ad was already scraped stop scraping in this page base\n",
    "            return len(rows_to_save_db)\n",
    "\n",
    "        next_page_tag = bsObj.find('a', {'data-lurker-detail':'next_page'})\n",
    "        next_page_url = next_page_tag['href'] if next_page_tag else None\n",
    "        if next_page_url:\n",
    "            return len(rows_to_save_db) + scrape_and_insert_data_olx(next_page_url,urls_already_access,page_number+1,_session,force_search=force_search)\n",
    "    else:\n",
    "        raise Exception(f\"Problem with Server in {url_end_point} page {page_number}\")\n",
    "    \n",
    "    # except Exception as e:\n",
    "    #         print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+ url_end_point)\n",
    "    #         print(str(e))\n",
    "    #         print(f'!!! Restart session and Wait {15*(qt_try**3)} seconds\\n\\n')\n",
    "    #         time.sleep(15*(qt_try**3))\n",
    "    #         _session = requests.Session()\n",
    "    #         if qt_try < 6:\n",
    "    #             return len(rows_to_save_db) + scrape_and_insert_data_olx(url_end_point,urls_already_access,page_number,_session,qt_try+1,force_search=force_search)\n",
    "    #         else:\n",
    "    #             print(f'!!! Error, qt attempts:{str(qt_try)} in:{url_end_point} page {page_number}')\n",
    "\n",
    "    return len(rows_to_save_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_ad_olx():\n",
    "    print('Scraping all ad data OLX')\n",
    "    start_urls_to_scraping  = banco_olx.get_urls_base_in_db(only_not_scraped=True)\n",
    "    if len(start_urls_to_scraping) == 0:\n",
    "        start_urls_to_scraping  = banco_olx.get_urls_base_in_db() # if all urls already scraped, update all data\n",
    "    rand_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=15))\n",
    "    for url in start_urls_to_scraping:\n",
    "        print(f\"Starting Scraping {url}\")\n",
    "        banco_olx.init_scraping_url_base(url,rand_string)\n",
    "        if banco_olx.verify_id_session(url,rand_string): # verify if session is the same (util for multiprocessing feature to use a cloud db)\n",
    "            current_reg_in_url = banco_olx.get_all_urls_ads(url)\n",
    "            qt_reg_add = scrape_and_insert_data_olx(url,current_reg_in_url)\n",
    "            banco_olx.end_scraping_url_base(url,qt_reg_add)\n",
    "            print(f\"End Scraping {url} - {qt_reg_add} new ads\")\n",
    "        else:\n",
    "            print(f\"End Scraping: Other process/computer is scraping {url}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_olx_page(url_end_point,urls_already_access,force_search=False):\n",
    "    list_result_data=[]\n",
    "    session = requests.Session()\n",
    "\n",
    "    def scrape_data_olx_page_retroactive(url_end_point,page_number=1,qt_try=1):\n",
    "        # print(f\"Scraping {url_end_point} page {page_number}\")\n",
    "        nonlocal session\n",
    "        query_Parameters = {'sf':1} if page_number == 1 else {} # get data in order of publication (THIS CASE IS POSSIBLE STOP SCRAPING IF SEE ONE AD ALREADY SCRAPED)\n",
    "        try:\n",
    "            resp = session.get(url_end_point, params=query_Parameters, headers=headers_olx, timeout=15)\n",
    "            bsObj = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            _problem = True if bsObj.find('title',text=\"Ops!!\") else False\n",
    "            if not _problem :\n",
    "                _url_end_point = url_end_point.split('?')[0]\n",
    "                all_row_scraped = get_data_from_olx_page(bsObj,_url_end_point,page_number)\n",
    "                rows_to_save_db = [*filter(lambda x: x['url_anuncio'] not in urls_already_access, all_row_scraped)] # add only new ads\n",
    "                list_result_data.extend(rows_to_save_db)\n",
    "\n",
    "                if len(rows_to_save_db) != len(all_row_scraped) and not force_search:  # if some ad was already scraped stop scraping in this page base\n",
    "                    return True\n",
    "\n",
    "                next_page_tag = bsObj.find('a', {'data-lurker-detail':'next_page'})\n",
    "                next_page_url = next_page_tag['href'] if next_page_tag else None\n",
    "                if next_page_url:\n",
    "                    return scrape_data_olx_page_retroactive(next_page_url,page_number+1,qt_try)\n",
    "            else:\n",
    "                raise Exception(f\"Problem with Server in {url_end_point} page {page_number}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "                print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+ url_end_point)\n",
    "                print(str(e))\n",
    "                print(f'!!! Restart session and Wait {15*(qt_try**3)} seconds\\n\\n')\n",
    "                time.sleep(15*(qt_try**3))\n",
    "                session = requests.Session()\n",
    "                if qt_try < 6:\n",
    "                    return scrape_data_olx_page_retroactive(url_end_point,page_number,qt_try+1)\n",
    "                else:\n",
    "                    print(f'!!! Error, qt attempts:{str(qt_try)} in:{url_end_point} page {page_number}')\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    scrape_data_olx_page_retroactive(url_end_point)\n",
    "    return list_result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultl = [[],[]]\n",
    "# erro = scrape_and_insert_data_olx('https://pi.olx.com.br/regiao-de-teresina-e-parnaiba/teresina/imoveis/venda/casas?o=93&sf=1')\n",
    "# jp2 = scrape_data_olx_page('https://ba.olx.com.br/grande-salvador/salvador/imoveis/venda/casas?o=100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_ad_olx_threads(qt_threads=10,verbose=False):\n",
    "    print('Scraping all ad data OLX')\n",
    "    start_urls_to_scraping  = banco_olx.get_urls_base_in_db(only_not_scraped=True)\n",
    "    \n",
    "    if len(start_urls_to_scraping) == 0:\n",
    "        print('All urls already scraped, updating all data')\n",
    "        start_urls_to_scraping  = banco_olx.get_urls_base_in_db() # if all urls already scraped, update all data and scrape all data again in order of publication\n",
    "    else:\n",
    "        random.shuffle(start_urls_to_scraping) # randomize the order of scraping (AVOID COLLISIONS for first scraping base)\n",
    "\n",
    "    rand_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=15))\n",
    "    def _scrape_data_olx_page(url,results,index,current_reg_in_url):\n",
    "        #is necessay to use this function because the get_retroactive_links_in_div function is not thread safe\n",
    "        results[index] = scrape_data_olx_page(url,current_reg_in_url)\n",
    "    \n",
    "    threads = [None] * qt_threads\n",
    "    results = [None] * qt_threads\n",
    "    qt_urls_base = len(start_urls_to_scraping)\n",
    "\n",
    "    print('Start Scraping all ad data OLX')\n",
    "    for index in range(0,qt_urls_base,qt_threads):\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                print(f\"Starting Scraping {start_urls_to_scraping[index+i]}\")\n",
    "                banco_olx.init_scraping_url_base(start_urls_to_scraping[index+i],rand_string)\n",
    "                if banco_olx.verify_id_session(start_urls_to_scraping[index+i],rand_string): # verify if other process/computer is scraping this url\n",
    "                    current_reg_in_url = banco_olx.get_all_urls_ads(start_urls_to_scraping[index+i])\n",
    "                    threads[i] = Thread(target= _scrape_data_olx_page, args=(start_urls_to_scraping[index+i],results,i,current_reg_in_url))\n",
    "                    threads[i].start()\n",
    "                else:\n",
    "                    warning  = f\"End Scraping: Other process/computer is scraping {start_urls_to_scraping[index+i]}\"\n",
    "                    print(warning)\n",
    "                    threads[i] = Thread(target= print, args=(''))\n",
    "                    threads[i].start()\n",
    "\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                threads[i].join()\n",
    "                if results[i]!=None:\n",
    "                    qt_new_ads = len(results[i])\n",
    "                    if qt_new_ads>0 :\n",
    "                        banco_olx.add_rows_anuncios_resumo(results[i]) # need add rows in the same thread to avoid concurrency problems\n",
    "                    banco_olx.end_scraping_url_base(start_urls_to_scraping[index+i],qt_new_ads)\n",
    "                    print(f\"End Scraping {start_urls_to_scraping[index+i]} - {qt_new_ads} new ads\")\n",
    " \n",
    "        threads = [None] * qt_threads\n",
    "        results = [None] * qt_threads\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_all_ad_olx_threads(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_all_ad_olx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_and_insert_data_olx('https://rj.olx.com.br/rio-de-janeiro-e-regiao/autos-e-pecas',urls_already_access=banco_olx.get_all_urls_ads('https://rj.olx.com.br/rio-de-janeiro-e-regiao/autos-e-pecas'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf242f9ae95059c186c6323b314d6698994e3ef345fd3978f42b2a4e2555921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
