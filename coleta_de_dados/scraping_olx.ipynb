{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "from functools import reduce\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackupApiResp:\n",
    "    def __init__(self,path_file_bk, max_days_to_valid_data = 720): \n",
    "        self.__con = sqlite3.connect(path_file_bk)\n",
    "        self.__con.row_factory = sqlite3.Row\n",
    "        self.days_to_expires = max_days_to_valid_data \n",
    "        self.today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        self.__init_db()\n",
    "    \n",
    "    def __init_db(self):\n",
    "        sql_table_def = (\n",
    "            'CREATE TABLE IF NOT EXISTS ' \n",
    "            'anuncios_resumo ('\n",
    "                'url_lista TEXT,'\n",
    "                'categoria_completa TEXT,'\n",
    "                'categoria_atual TEXT,'\n",
    "                'localizacao_completa TEXT,'\n",
    "                'data_coleta_dados DATETIME,'\n",
    "                'titulo_anuncio TEXT,'\n",
    "                'preco_anuncio FLOAT,'\n",
    "                'anuncio_profissional INTEGER,'\n",
    "                'url_anuncio TEXT,'\n",
    "                'data_publicacao_anuncio DATETIME'\n",
    "            ');'\n",
    "        )\n",
    "\n",
    "        sql_index1_def = (\n",
    "            'CREATE INDEX IF NOT EXISTS '\n",
    "            'index_url_anuncio_ar ON '\n",
    "            'anuncios_resumo ('\n",
    "                'url_anuncio ASC'\n",
    "            ');'\n",
    "        )\n",
    "\n",
    "        sql_index2_def = (\n",
    "            'CREATE INDEX IF NOT EXISTS '\n",
    "            'index_url_lista_ar ON '\n",
    "            'anuncios_resumo ('\n",
    "                'url_lista ASC'\n",
    "            ');'\n",
    "        )\n",
    "\n",
    "        self.__con.execute(sql_table_def)\n",
    "        self.__con.execute(sql_index1_def)\n",
    "        self.__con.execute(sql_index2_def)\n",
    "        self.__con.commit()\n",
    "\n",
    "    def add_row_anuncios_resumo(self,url_lista,categoria_completa,categoria_atual,localizacao_completa,titulo_anuncio,preco_anuncio, anuncio_profissional,url_anuncio,data_publicacao_anuncio):\n",
    "        sql_insert_data = (\n",
    "            \"INSERT INTO anuncios_resumo (url_lista,categoria_completa,categoria_atual,localizacao_completa,data_coleta_dados,titulo_anuncio,preco_anuncio, anuncio_profissional,url_anuncio,data_publicacao_anuncio) \"\n",
    "            \"VALUES (\"\n",
    "                f\"'{url_lista}',\"\n",
    "                f\"'{categoria_completa}',\"\n",
    "                f\"'{categoria_atual}',\"\n",
    "                f\"'{localizacao_completa}',\"\n",
    "                f\"'{self.today}',\"\n",
    "                f\"'{titulo_anuncio}',\"\n",
    "                f\"{preco_anuncio},\"\n",
    "                f\"{anuncio_profissional},\"\n",
    "                f\"'{url_anuncio}',\"\n",
    "                f\"'{data_publicacao_anuncio}' \"\n",
    "            \");\"\n",
    "        )\n",
    "        self.__con.execute(sql_insert_data)\n",
    "        self.__con.commit()\n",
    "    \n",
    "    def date_is_valid(self,date:str):\n",
    "        data_age_days = (datetime.now()-datetime.fromisoformat(date)).days\n",
    "        return (data_age_days <= self.days_to_expires)\n",
    "\n",
    "    def has_url_ad_in_anuncio_resumo(self,url_ad):\n",
    "        sql = f\"\"\" select url_anuncio from anuncios_resumo\n",
    "                    where url_anuncio = '{url_ad}'\n",
    "                    limit 1 \"\"\"\n",
    "\n",
    "        result = self.__con.execute(sql).fetchone()\n",
    "        return True if result else False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_verbose_function(verbose):\n",
    "    if verbose:\n",
    "        return print\n",
    "    else:\n",
    "        return lambda x: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_olx_to_datetime_str(texto_data):\n",
    "    MONTHS = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4,  'mai': 5,  'jun': 6,\n",
    "          'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}\n",
    "    data, hora = texto_data.lower().split(',')\n",
    "    data_padrao = \"\"\n",
    "    \n",
    "    if 'ontem' == data :\n",
    "        data_padrao = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    elif 'hoje' == data :\n",
    "        data_padrao = datetime.now().strftime('%Y-%m-%d')\n",
    "    else:    \n",
    "        dataP = data.split(' ')\n",
    "        dia_mes = int(dataP[0][0:2])\n",
    "        mes = MONTHS[dataP[1]]\n",
    "        ano_atual = int(datetime.now().strftime('%Y'))\n",
    "        mes_atual = int(datetime.now().strftime('%m'))\n",
    "        ano = ano_atual if mes_atual >= mes else ano_atual - 1\n",
    "        data_padrao = datetime(year=ano, month=mes, day=dia_mes).strftime('%Y-%m-%d')\n",
    "    \n",
    "    return data_padrao + hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Teste\n",
    "# backup = BackupApiResp(\"./banco_backup.db\")\n",
    "# backup.add_row_anuncios_resumo('www.teste','hobbys;livros','livros','Brasil;Minas;31','mÃ¡gico de oz',200.98, 1,'www.teste2','2022-08-30 15:21')\n",
    "# backup.has_url_ad_in_anuncio_resumo('www.teste2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_olx =  { \n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "                \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "                \"origin\": \"https://olx.com.br/\",\n",
    "                \"referer\": \"https://olx.com.br/\",\n",
    "                \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"96\\\", \\\"Google Chrome\\\";v=\\\"96\\\"\",\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\n",
    "                \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "                \"sec-fetch-dest\": \"empty\",\n",
    "                \"sec-fetch-mode\": \"cors\",\n",
    "                \"sec-fetch-site\": \"same-site\",\n",
    "                \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retroactive_links_in_div(div_class,url,verbose=True,only_endpoints=False):\n",
    "    \"\"\"\n",
    "    this code is used to get the retroactive links in the div class\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    all_url_visited = []\n",
    "    end_links = []\n",
    "    error_links = []\n",
    "    iprint = print_verbose_function(verbose)\n",
    "\n",
    "    def get_links_in_current_div(url_base,suffix='',qt_try=1):\n",
    "        nonlocal session\n",
    "        if url_base not in all_url_visited:\n",
    "            all_url_visited.append(url_base)   \n",
    "        iprint(\"*****url_base: \"+ url_base)\n",
    "        \n",
    "        try:\n",
    "            resp = session.get(url_base+suffix, headers=headers_olx, timeout=15)\n",
    "            bsObj = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            items = bsObj.find(\"div\", {\"class\":div_class})\n",
    "            qt_reg_adds = 0\n",
    "            if items:\n",
    "                links_in_div = [*map(lambda x: x['href'], items.find_all(\"a\", href=True))]\n",
    "                \n",
    "                for link in links_in_div:\n",
    "                    if link not in all_url_visited:\n",
    "                        iprint('+Redirecionamento:'+url_base+'->' + link)\n",
    "                        get_links_in_current_div(link,'/')\n",
    "                        qt_reg_adds+=1\n",
    "                \n",
    "            else:\n",
    "                print('warning-no-class:' + url_base)\n",
    "            \n",
    "            if qt_reg_adds == 0:\n",
    "                if url_base not in end_links:\n",
    "                    iprint('----end_link:'+url_base) \n",
    "                    end_links.append(url_base)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+url_base)\n",
    "            print(str(e))\n",
    "            print('!!! Restart session and Wait 10 seconds\\n\\n')\n",
    "            time.sleep(10)\n",
    "            session = requests.Session()\n",
    "            if qt_try < 6:\n",
    "                get_links_in_current_div(url_base,suffix,qt_try+1)\n",
    "            else:\n",
    "                print('!!! Error, qt attempts:'+str(qt_try)+' in:'+url_base)\n",
    "                error_links.append(url_base)\n",
    "\n",
    "    get_links_in_current_div(url)       \n",
    "    \n",
    "    return all_url_visited if only_endpoints == False else end_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _test_endlink = get_retroactive_links_in_div('sc-1ncgzjx-0','https://mg.olx.com.br/belo-horizonte-e-regiao/imoveis/venda',True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_category_urls():\n",
    "    \"\"\"\n",
    "    Return only the most specific category links \n",
    "    *(general categories are not returned and yours advertisements are included in specific categories)\n",
    "    * is necessary get specific categories because the general categories are limited in 5k advertisements per location\n",
    "    \"\"\"\n",
    "    url_base = \"https://www.olx.com.br/brasil\"\n",
    "    class_category_menu = \"jx24x3-2\"\n",
    "\n",
    "    all_url_visited = get_retroactive_links_in_div(class_category_menu,url_base) # end_links = False -> because there are siblings links in categories menu\n",
    "\n",
    "    most_specific_category = []\n",
    "\n",
    "    # Is necessary threat all links using Regex to get the most specific category\n",
    "    for link_ref in all_url_visited:\n",
    "        add = True\n",
    "        for link_comp in all_url_visited:\n",
    "            if re.match(link_ref, link_comp) and link_ref != link_comp:\n",
    "                add = False\n",
    "                break\n",
    "        if add:\n",
    "            most_specific_category.append(link_ref)\n",
    "\n",
    "    most_specific_category.remove(url_base)       \n",
    "    \n",
    "    return  most_specific_category   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_urls_by_location_threads(urls_base,qt_threads=10,verbose=False):\n",
    "    \"\"\"\n",
    "    Return a list of urls to be scraped (inside a specific region)\n",
    "    Is necessary to use threads because the OLX website has a lot of categories and the scraping is slow\n",
    "    \"\"\"\n",
    "    def _get_retroactive_links_in_div(div_class,url,results,index):\n",
    "        #is necessay to use this function because the get_retroactive_links_in_div function is not thread safe\n",
    "        results[index] = get_retroactive_links_in_div(div_class,url,verbose,True)\n",
    "    \n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\n",
    "    start_urls = []\n",
    "    threads = [None] * qt_threads\n",
    "    results = [None] * qt_threads\n",
    "    qt_urls_base = len(urls_base)\n",
    "    print('Start get_start_urls_scraping_threads - This Function take many time to finish')\n",
    "    for index in range(0,qt_urls_base,qt_threads):\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                threads[i] = Thread(target=_get_retroactive_links_in_div, args=(class_location_menu,urls_base[index+i],results,i))\n",
    "                threads[i].start()\n",
    "        for i in range(qt_threads):\n",
    "            if (index+i) < qt_urls_base:\n",
    "                threads[i].join()\n",
    "                print(f\"Category {urls_base[index+i]} ended: {index+i+1} / {qt_urls_base}\")\n",
    "                start_urls.extend(results[i])\n",
    "        # start_urls = sum(results,start_urls)\n",
    "        threads = [None] * qt_threads\n",
    "        results = [None] * qt_threads\n",
    "\n",
    "    return [*filter(lambda x: x != None, start_urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test = get_start_urls_scraping_threads(['https://ac.olx.com.br/hobbies-e-colecoes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_urls_by_location(urls_base,verbose=False):\n",
    "    \"\"\"\n",
    "    Return a list of urls to be scraped (inside a specific region)\n",
    "    \"\"\"\n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\n",
    "    all_url_visited = []\n",
    "    for url_base in urls_base:\n",
    "      qt_url_base = len(urls_base)\n",
    "      percetil_end = len(all_url_visited)/qt_url_base*100\n",
    "      print(f'Percentual Concluido:{percetil_end:.2f}% - {len(all_url_visited)} de {qt_url_base}')\n",
    "      all_url_visited.extend(get_retroactive_links_in_div(class_location_menu,url_base,verbose,True))\n",
    "    return all_url_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_category_urls = []\n",
    "if not Path('./unique_category_urls.pkl').is_file():\n",
    "    unique_category_urls = get_unique_category_urls()\n",
    "    with open('./unique_category_urls.pkl', 'wb') as file:\n",
    "        pickle.dump(unique_category_urls, file)\n",
    "else :\n",
    "    with open('./unique_category_urls.pkl', 'rb') as file:\n",
    "        unique_category_urls = pickle.load(file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls_scraping = []\n",
    "if not Path('./start_urls_scraping.pkl').is_file():\n",
    "    start_urls_scraping = get_start_urls_by_location_threads(unique_category_urls,12)\n",
    "    with open('./start_urls_scraping.pkl', 'wb') as file:\n",
    "        pickle.dump(start_urls_scraping, file) \n",
    "else :\n",
    "    with open('./start_urls_scraping.pkl', 'rb') as file:\n",
    "        start_urls_scraping = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf242f9ae95059c186c6323b314d6698994e3ef345fd3978f42b2a4e2555921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
