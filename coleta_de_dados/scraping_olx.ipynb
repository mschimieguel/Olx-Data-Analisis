{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time #usado para esperar um certo tempo para executar um comando\r\n",
    "from threading import Thread #usado para executar funções ao mesmo tempo\r\n",
    "import re #regex\r\n",
    "import pickle #usado para salvar variáveis na memória hd\r\n",
    "from pathlib import Path #usado para acessar pastas na memória\r\n",
    "import sqlite3 #conexão com o banco sqlite\r\n",
    "import string \r\n",
    "import random\r\n",
    "import requests #usado para fazer requisições http\r\n",
    "from bs4 import BeautifulSoup #transformar texto html em estrutura pyhton\r\n",
    "import pandas as pd\r\n",
    "from datetime import datetime, timedelta #usado para tratar datas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "class BackupApiResp:\r\n",
    "    def __init__(self,path_file_bk): \r\n",
    "        self.__con = sqlite3.connect(path_file_bk) #inicia a conexão com o banco\r\n",
    "        self.__con.row_factory = sqlite3.Row \r\n",
    "        self.today = datetime.now().strftime(\"%Y-%m-%d %H:%M\") #define data atual no padrão ano-mes-dia horas-minutos\r\n",
    "        self.__init_db() #chama a função que inicia o banco de dados\r\n",
    "    \r\n",
    "    def __treat_text_dict_to_sql(text_dict):\r\n",
    "        for key in text_dict:\r\n",
    "            text_dict[key] = text_dict[key].replace(\"'\",\"''\")\r\n",
    "        return text_dict\r\n",
    "    def __init_db(self): \r\n",
    "        '''cria o banco executando a query abaixo'''\r\n",
    "        sql_table_def = (\r\n",
    "            'CREATE TABLE IF NOT EXISTS ' \r\n",
    "            'anuncios_resumo ('\r\n",
    "                'url_lista TEXT,'\r\n",
    "                'numero_pagina_lista INTEGER,'\r\n",
    "                'url_anuncio TEXT,'\r\n",
    "                'categoria_completa TEXT,'\r\n",
    "                'categoria_atual TEXT,'\r\n",
    "                'titulo_anuncio TEXT,'\r\n",
    "                'detalhes_anuncio TEXT,'\r\n",
    "                'preco_anuncio FLOAT,'\r\n",
    "                'url_img_principal TEXT,'\r\n",
    "                'qt_img_anuncio INTEGER,'\r\n",
    "                'localizacao_completa TEXT,'\r\n",
    "                'localizacao_complemento TEXT,'\r\n",
    "                'anuncio_profissional INTEGER,'\r\n",
    "                'data_publicacao_anuncio DATETIME,'\r\n",
    "                'data_coleta_dados DATETIME'\r\n",
    "            ');' \r\n",
    "        ) #cria a tabela anuncios_resumo se ela não existir, com as colunas informadas\r\n",
    "\r\n",
    "        sql_index1_def = (\r\n",
    "            'CREATE INDEX IF NOT EXISTS '\r\n",
    "            'index_url_anuncio_ar ON '\r\n",
    "            'anuncios_resumo ('\r\n",
    "                'url_anuncio ASC'\r\n",
    "            ');'\r\n",
    "        ) #cria um índice com a url_anuncio\r\n",
    "\r\n",
    "        sql_index2_def = (\r\n",
    "            'CREATE INDEX IF NOT EXISTS '\r\n",
    "            'index_url_lista_ar ON '\r\n",
    "            'anuncios_resumo ('\r\n",
    "                'url_lista ASC'\r\n",
    "            ');' \r\n",
    "        ) #cria um índice com a url_lista\r\n",
    "        sql_index3_def = (\r\n",
    "            'CREATE INDEX IF NOT EXISTS '\r\n",
    "            'index_categoria_titulo_url ON '\r\n",
    "            'anuncios_resumo ('\r\n",
    "                'categoria_atual ASC,'\r\n",
    "                'titulo_anuncio ASC,'\r\n",
    "                'url_anuncio ASC'\r\n",
    "            ');' \r\n",
    "        )\r\n",
    "        sql_index_control = (\r\n",
    "            'CREATE INDEX IF NOT EXISTS '\r\n",
    "            'index_table_control ON '\r\n",
    "            'scraping_control ('\r\n",
    "                'url_base ASC'\r\n",
    "            ');'\r\n",
    "        ) #cria um índice com a url_base\r\n",
    "        sql_table_control = (\r\n",
    "            'CREATE TABLE IF NOT EXISTS ' \r\n",
    "            'scraping_control ('\r\n",
    "                'url_base TEXT,'\r\n",
    "                'qt_last_scraping INTEGER,'\r\n",
    "                'id_session TEXT,'\r\n",
    "                'date_init_scraping DATETIME,'\r\n",
    "                'date_end_scraping DATETIME'\r\n",
    "            ');'\r\n",
    "        )\r\n",
    "        self.__con.execute(sql_table_def) #executa as querys sobre a conexão iniciada \r\n",
    "        self.__con.execute(sql_index1_def)\r\n",
    "        self.__con.execute(sql_index2_def)\r\n",
    "        self.__con.execute(sql_index3_def)\r\n",
    "        self.__con.execute(sql_table_control)\r\n",
    "        self.__con.execute(sql_index_control)\r\n",
    "        self.__con.commit()\r\n",
    "\r\n",
    "    def add_rows_anuncios_resumo(self,rows):\r\n",
    "        '''Faz a inserção de linhas na tabela'''\r\n",
    "        if isinstance(rows,list):\r\n",
    "            if len(rows) > 0 and isinstance(rows[0],dict):\r\n",
    "                sql_insert_data = \"\"\"INSERT INTO anuncios_resumo (\r\n",
    "                                                    url_lista,\r\n",
    "                                                    numero_pagina_lista,\r\n",
    "                                                    url_anuncio,\r\n",
    "                                                    categoria_completa,\r\n",
    "                                                    categoria_atual,\r\n",
    "                                                    titulo_anuncio,\r\n",
    "                                                    detalhes_anuncio,\r\n",
    "                                                    preco_anuncio,\r\n",
    "                                                    url_img_principal,\r\n",
    "                                                    qt_img_anuncio,\r\n",
    "                                                    localizacao_completa,\r\n",
    "                                                    localizacao_complemento,\r\n",
    "                                                    anuncio_profissional,\r\n",
    "                                                    data_publicacao_anuncio,\r\n",
    "                                                    data_coleta_dados ) VALUES\"\"\"\r\n",
    "                sql_rows = []\r\n",
    "                for row in rows:\r\n",
    "                    row = BackupApiResp.__treat_text_dict_to_sql(row)\r\n",
    "                    sql_rows.append(f\"\"\"('{row.get('url_lista',\"erro\")}',\r\n",
    "                                          {row.get('numero_pagina_lista',\"-1\")},\r\n",
    "                                         '{row.get('url_anuncio',\"erro\")}',\r\n",
    "                                         '{row.get('categoria_completa',\"erro\")}',\r\n",
    "                                         '{row.get('categoria_atual',\"erro\")}',\r\n",
    "                                         '{row.get('titulo_anuncio',\"erro\")}',\r\n",
    "                                         '{row.get('detalhes_anuncio',\"erro\")}',\r\n",
    "                                          {row.get('preco_anuncio',\"-1\")},\r\n",
    "                                         '{row.get('url_img_principal',\"erro\")}',\r\n",
    "                                          {row.get('qt_img_anuncio',\"-1\")},\r\n",
    "                                         '{row.get('localizacao_completa',\"erro\")}',\r\n",
    "                                         '{row.get('localizacao_complemento',\"erro\")}',\r\n",
    "                                          {row.get('anuncio_profissional',\"-1\")},\r\n",
    "                                         '{row.get('data_publicacao_anuncio',self.today)}',\r\n",
    "                                         '{self.today}')\"\"\")\r\n",
    "                sql_insert_data += ','.join(sql_rows) + ';'\r\n",
    "                self.__con.execute(sql_insert_data)\r\n",
    "                self.__con.commit()\r\n",
    "    \r\n",
    "    def add_url_scraping_control(self,url_base):\r\n",
    "        sql_insert_data = (\r\n",
    "            \"INSERT INTO scraping_control (url_base) \"\r\n",
    "            \"VALUES (\"\r\n",
    "                f\"'{url_base}'\"\r\n",
    "            \");\"\r\n",
    "        )\r\n",
    "        self.__con.execute(sql_insert_data)\r\n",
    "        self.__con.commit()\r\n",
    "    \r\n",
    "    def init_scraping_url_base(self,url_base,id_session='-'):\r\n",
    "        sql_insert_data = f\"\"\"UPDATE scraping_control\r\n",
    "                                SET date_init_scraping = '{datetime.now().strftime(\"%Y-%m-%d %H:%M\")}',\r\n",
    "                                    id_session = '{id_session}',\r\n",
    "                                    date_end_scraping = NULL\r\n",
    "                              WHERE url_base = '{url_base}' AND \r\n",
    "                              ((id_session is NULL) OR \r\n",
    "                              COALESCE(julianday('now') - julianday(date_init_scraping),99)>1)\r\n",
    "                              \"\"\"\r\n",
    "     \r\n",
    "        self.__con.execute(sql_insert_data)\r\n",
    "        self.__con.commit()\r\n",
    "    \r\n",
    "    def verify_id_session(self,url_base,id_session): #util if use a cloud db\r\n",
    "        sql =  f\"SELECT url_base FROM scraping_control where url_base = '{url_base}' and id_session = '{id_session}'\"\r\n",
    "        result = self.__con.execute(sql).fetchone()\r\n",
    "        return True if result else False  \r\n",
    "    \r\n",
    "    def get_urls_base_in_db(self,only_not_scraped = False):\r\n",
    "        sql_data = f\"\"\"SELECT url_base FROM scraping_control\"\"\"\r\n",
    "        sql_data += f\"\"\" where date_end_scraping is null order by id_session, date_init_scraping desc \"\"\" if only_not_scraped else \" order by id_session, qt_last_scraping desc\"\r\n",
    "     \r\n",
    "        result = self.__con.execute(sql_data).fetchall()\r\n",
    "        return [row['url_base'] for row in result]\r\n",
    "        \r\n",
    "    def end_scraping_url_base(self,url_base,qt_last_scraping):\r\n",
    "        sql_insert_data = f\"\"\"UPDATE scraping_control\r\n",
    "                                SET date_end_scraping = '{datetime.now().strftime(\"%Y-%m-%d %H:%M\")}',\r\n",
    "                                    qt_last_scraping = {qt_last_scraping},\r\n",
    "                                    id_session = NULL\r\n",
    "                              WHERE url_base = '{url_base}' \"\"\"\r\n",
    "     \r\n",
    "        self.__con.execute(sql_insert_data)\r\n",
    "        self.__con.commit()\r\n",
    "\r\n",
    "    def get_all_urls_ads(self,url_lista):\r\n",
    "        sql_data = f\"\"\"SELECT url_anuncio FROM anuncios_resumo where url_lista = '{url_lista}'\"\"\"\r\n",
    "        result = self.__con.execute(sql_data).fetchall()\r\n",
    "        return [row['url_anuncio'] for row in result]\r\n",
    "\r\n",
    "    def has_url_ad_in_anuncio_resumo(self,url_ad):\r\n",
    "        sql = f\"\"\" select url_anuncio from anuncios_resumo\r\n",
    "                    where url_anuncio = '{url_ad}'\r\n",
    "                    limit 1 \"\"\"\r\n",
    "\r\n",
    "        result = self.__con.execute(sql).fetchone()\r\n",
    "        return True if result else False\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def print_verbose_function(verbose):\r\n",
    "    if verbose:\r\n",
    "        return print\r\n",
    "    else:\r\n",
    "        return lambda x: None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def convert_date_olx_to_datetime_str(texto_data):\r\n",
    "    MONTHS = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4,  'mai': 5,  'jun': 6,\r\n",
    "          'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}\r\n",
    "    data, hora = texto_data.lower().split(',')\r\n",
    "    data_padrao = \"\"\r\n",
    "    \r\n",
    "    if 'ontem' == data :\r\n",
    "        data_padrao = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\r\n",
    "    elif 'hoje' == data :\r\n",
    "        data_padrao = datetime.now().strftime('%Y-%m-%d')\r\n",
    "    else:    \r\n",
    "        dataP = data.split(' ')\r\n",
    "        dia_mes = int(dataP[0][0:2])\r\n",
    "        mes = MONTHS[dataP[1]]\r\n",
    "        ano_atual = int(datetime.now().strftime('%Y'))\r\n",
    "        mes_atual = int(datetime.now().strftime('%m'))\r\n",
    "        ano = ano_atual if mes_atual >= mes else ano_atual - 1\r\n",
    "        try:\r\n",
    "            data_padrao = datetime(year=ano, month=mes, day=dia_mes).strftime('%Y-%m-%d')\r\n",
    "        except:\r\n",
    "            data_padrao = datetime(year=2000, month=1, day=1).strftime('%Y-%m-%d')\r\n",
    "  \r\n",
    "    \r\n",
    "    return data_padrao + hora"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "headers_olx =  { #Network -> Request Headers\r\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\r\n",
    "                \"accept-encoding\": \"gzip, deflate, br\",\r\n",
    "                \"accept-language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\r\n",
    "                \"origin\": \"https://olx.com.br/\",\r\n",
    "                \"referer\": \"https://olx.com.br/\",\r\n",
    "                \"sec-ch-ua\": \"\\\" Not A;Brand\\\";v=\\\"99\\\", \\\"Chromium\\\";v=\\\"96\\\", \\\"Google Chrome\\\";v=\\\"96\\\"\",\r\n",
    "                \"sec-ch-ua-mobile\": \"?0\",\r\n",
    "                \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\r\n",
    "                \"sec-fetch-dest\": \"empty\",\r\n",
    "                \"sec-fetch-mode\": \"cors\",\r\n",
    "                \"sec-fetch-site\": \"same-site\",\r\n",
    "                \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36\"\r\n",
    "                }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_retroactive_links_in_div(div_class,url,verbose=True,only_endpoints=False):\r\n",
    "    \"\"\"\r\n",
    "    this code is used to get the retroactive links in the div class\r\n",
    "    \"\"\"\r\n",
    "    session = requests.Session()\r\n",
    "    all_url_visited = []\r\n",
    "    end_links = []\r\n",
    "    error_links = []\r\n",
    "    iprint = print_verbose_function(verbose)\r\n",
    "\r\n",
    "    def get_links_in_current_div(url_base,suffix='',qt_try=1):\r\n",
    "        nonlocal session\r\n",
    "        if url_base not in all_url_visited:\r\n",
    "            all_url_visited.append(url_base)   \r\n",
    "        iprint(\"*****url_base: \"+ url_base)\r\n",
    "        \r\n",
    "        try:\r\n",
    "            resp = session.get(url_base+suffix, headers=headers_olx, timeout=15)\r\n",
    "            bsObj = BeautifulSoup(resp.text, \"html.parser\")\r\n",
    "            items = bsObj.find(\"div\", {\"class\":div_class})\r\n",
    "            qt_reg_adds = 0\r\n",
    "            if items:\r\n",
    "                links_in_div = [*map(lambda x: x['href'], items.find_all(\"a\", href=True))]\r\n",
    "                \r\n",
    "                for link in links_in_div:\r\n",
    "                    if link not in all_url_visited:\r\n",
    "                        iprint('+Redirecionamento:'+url_base+'->' + link)\r\n",
    "                        get_links_in_current_div(link,'/')\r\n",
    "                        qt_reg_adds+=1\r\n",
    "                \r\n",
    "            else:\r\n",
    "                print('warning-no-class:' + url_base)\r\n",
    "            \r\n",
    "            if qt_reg_adds == 0:\r\n",
    "                if url_base not in end_links:\r\n",
    "                    iprint('----end_link:'+url_base) \r\n",
    "                    end_links.append(url_base)\r\n",
    "        \r\n",
    "        except Exception as e:\r\n",
    "            print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+url_base)\r\n",
    "            print(str(e))\r\n",
    "            print('!!! Restart session and Wait 10 seconds\\n\\n')\r\n",
    "            time.sleep(10)\r\n",
    "            session = requests.Session()\r\n",
    "            if qt_try < 6:\r\n",
    "                get_links_in_current_div(url_base,suffix,qt_try+1)\r\n",
    "            else:\r\n",
    "                print('!!! Error, qt attempts:'+str(qt_try)+' in:'+url_base)\r\n",
    "                error_links.append(url_base)\r\n",
    "\r\n",
    "    get_links_in_current_div(url)       \r\n",
    "    \r\n",
    "    return all_url_visited if only_endpoints == False else end_links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# _test_endlink = get_retroactive_links_in_div('sc-1ncgzjx-0','https://mg.olx.com.br/belo-horizonte-e-regiao/imoveis/venda',True,True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_unique_category_urls():\r\n",
    "    \"\"\"\r\n",
    "    Return only the most specific category links \r\n",
    "    *(general categories are not returned and yours advertisements are included in specific categories)\r\n",
    "    * is necessary get specific categories because the general categories are limited in 5k advertisements per location\r\n",
    "    \"\"\"\r\n",
    "    url_base = \"https://www.olx.com.br/brasil\"\r\n",
    "    class_category_menu = \"jx24x3-2\"\r\n",
    "\r\n",
    "    all_url_visited = get_retroactive_links_in_div(class_category_menu,url_base) # end_links = False -> because there are siblings links in categories menu\r\n",
    "\r\n",
    "    most_specific_category = []\r\n",
    "\r\n",
    "    # Is necessary threat all links using Regex to get the most specific category\r\n",
    "    for link_ref in all_url_visited:\r\n",
    "        add = True\r\n",
    "        for link_comp in all_url_visited:\r\n",
    "            if re.match(link_ref, link_comp) and link_ref != link_comp:\r\n",
    "                add = False\r\n",
    "                break\r\n",
    "        if add:\r\n",
    "            most_specific_category.append(link_ref)\r\n",
    "\r\n",
    "    most_specific_category.remove(url_base)       \r\n",
    "    \r\n",
    "    return  most_specific_category   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_start_urls_scraping_threads(urls_base,qt_threads=10,verbose=False):\r\n",
    "    \"\"\"\r\n",
    "    Return a list of urls to be scraped (inside a specific region)\r\n",
    "    Is necessary to use threads because the OLX website has a lot of categories and the scraping is slow\r\n",
    "    \"\"\"\r\n",
    "    def _get_retroactive_links_in_div(div_class,url,results,index):\r\n",
    "        #is necessay to use this function because the get_retroactive_links_in_div function is not thread safe\r\n",
    "        results[index] = get_retroactive_links_in_div(div_class,url,verbose,True)\r\n",
    "    \r\n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\r\n",
    "    start_urls = []\r\n",
    "    threads = [None] * qt_threads\r\n",
    "    results = [None] * qt_threads\r\n",
    "    qt_urls_base = len(urls_base)\r\n",
    "    print('Start get_start_urls_scraping_threads - This Function take many time to finish')\r\n",
    "    for index in range(0,qt_urls_base,qt_threads):\r\n",
    "        for i in range(qt_threads):\r\n",
    "            if (index+i) < qt_urls_base:\r\n",
    "                threads[i] = Thread(target=_get_retroactive_links_in_div, args=(class_location_menu,urls_base[index+i],results,i))\r\n",
    "                threads[i].start()\r\n",
    "        for i in range(qt_threads):\r\n",
    "            if (index+i) < qt_urls_base:\r\n",
    "                threads[i].join()\r\n",
    "                print(f\"Category {urls_base[index+i]} ended: {index+i+1} / {qt_urls_base}\")\r\n",
    "                start_urls.extend(results[i])\r\n",
    "        # start_urls = sum(results,start_urls)\r\n",
    "        threads = [None] * qt_threads\r\n",
    "        results = [None] * qt_threads\r\n",
    "\r\n",
    "    return [*filter(lambda x: x != None, start_urls)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Test = get_start_urls_scraping_threads(['https://ac.olx.com.br/hobbies-e-colecoes'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_start_urls_scraping(urls_base,vebose=False):\r\n",
    "    \"\"\"\r\n",
    "    Return a list of urls to be scraped (inside a specific region)\r\n",
    "    \"\"\"\r\n",
    "    class_location_menu = \"sc-1ncgzjx-0\"\r\n",
    "    all_url_visited = []\r\n",
    "    for url_base in urls_base:\r\n",
    "      qt_url_base = len(urls_base)\r\n",
    "      percetil_end = len(all_url_visited)/qt_url_base*100\r\n",
    "      print(f'Percentual Concluido:{percetil_end:.2f}% - {len(all_url_visited)} de {qt_url_base}')\r\n",
    "      all_url_visited.extend(get_retroactive_links_in_div(class_location_menu,url_base,vebose,True))\r\n",
    "    return all_url_visited"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "unique_category_urls = []\r\n",
    "if not Path('./unique_category_urls.pkl').is_file():\r\n",
    "    unique_category_urls = get_unique_category_urls()\r\n",
    "    with open('./unique_category_urls.pkl', 'wb') as file:\r\n",
    "        pickle.dump(unique_category_urls, file)\r\n",
    "else :\r\n",
    "    with open('./unique_category_urls.pkl', 'rb') as file:\r\n",
    "        unique_category_urls = pickle.load(file)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "start_urls_scraping = []\r\n",
    "if not Path('./start_urls_scraping.pkl').is_file():\r\n",
    "    start_urls_scraping = get_start_urls_scraping_threads(unique_category_urls,12)\r\n",
    "    with open('./start_urls_scraping.pkl', 'wb') as file:\r\n",
    "        pickle.dump(start_urls_scraping, file) \r\n",
    "else :\r\n",
    "    with open('./start_urls_scraping.pkl', 'rb') as file:\r\n",
    "        start_urls_scraping = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "banco_olx = BackupApiResp(\"./banco_scraping_olx.db\")\r\n",
    "all_urls = banco_olx.get_urls_base_in_db()\r\n",
    "urls_to_add_db = [*filter(lambda x: x not in all_urls, start_urls_scraping)]\r\n",
    "# add new urls to db for scrapping\r\n",
    "for url in urls_to_add_db:\r\n",
    "    banco_olx.add_url_scraping_control(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def get_data_from_olx_page(bsObj,list_url,page_number):\r\n",
    "    \r\n",
    "    div_category = bsObj.find(\"div\", {\"class\": \"otEye\"})\r\n",
    "    list_category = div_category.findAll(text=True) if div_category else []\r\n",
    "    current_category = list_category[-1] if len(list_category) > 0 else 'Error'\r\n",
    "    complete_category = ';'.join(list_category[1:]) if len(list_category) > 0 else 'Error'\r\n",
    "    div_location = bsObj.find(\"div\", {\"class\": \"sc-gPWkxV UqLlM\"})\r\n",
    "    complete_location = div_location.text.replace('>',';') if div_location else ''\r\n",
    "    ad_list = bsObj.findAll(\"li\", {\"class\": \"sc-1fcmfeb-2\"})\r\n",
    "    rows_to_save_db = []\r\n",
    "    for ad_resume in ad_list:\r\n",
    "\r\n",
    "        adv_url_tag = ad_resume.find(\"a\",href=True)\r\n",
    "        if adv_url_tag:\r\n",
    "            thumb_img_tag = adv_url_tag.find(\"img\",src=True)\r\n",
    "            qt_img_tag = ad_resume.find(\"span\",{\"class\":\"bYaEay\"})\r\n",
    "            desc_tag = ad_resume.find(\"h2\",text=True) \r\n",
    "            price_tag = ad_resume.find(\"span\",{\"class\":\"kHeyHD\"})\r\n",
    "            detail_tag = ad_resume.find(\"div\",{\"class\":\"jEDFNq\"})\r\n",
    "            address_detail_tag = ad_resume.find(\"span\",{\"class\":\"iDvjkv\"})\r\n",
    "            type_seller_tag = ad_resume.find(\"span\",{\"class\":\"jGYopB\"})\r\n",
    "            date_tag = ad_resume.find(\"span\",{\"class\":\"javKJU\"})\r\n",
    "            price = price_tag.text if price_tag else ''\r\n",
    "            \r\n",
    "            data_to_insert = {\r\n",
    "                                'url_lista': list_url,\r\n",
    "                                'numero_pagina_lista': str(page_number),\r\n",
    "                                'url_anuncio': adv_url_tag['href'] if adv_url_tag else '',\r\n",
    "                                'categoria_completa': complete_category,\r\n",
    "                                'categoria_atual': current_category,\r\n",
    "                                'titulo_anuncio': desc_tag.text if desc_tag else '',\r\n",
    "                                'detalhes_anuncio': detail_tag.getText() if detail_tag else '',\r\n",
    "                                'preco_anuncio': re.sub('[^0-9,]','',price).replace(',','.') if price else 'NULL',\r\n",
    "                                'url_img_principal': thumb_img_tag['src'] if thumb_img_tag else '',\r\n",
    "                                'qt_img_anuncio': qt_img_tag.text if qt_img_tag else '0',\r\n",
    "                                'localizacao_completa': complete_location,\r\n",
    "                                'localizacao_complemento': address_detail_tag.text if address_detail_tag else '',\r\n",
    "                                'anuncio_profissional':'1' if type_seller_tag else '0',\r\n",
    "                                'data_publicacao_anuncio': convert_date_olx_to_datetime_str(date_tag.text) if date_tag else datetime.now().strftime(\"%Y-%m-%d %H:%M\")\r\n",
    "                            }\r\n",
    "            rows_to_save_db.append(data_to_insert)\r\n",
    "    \r\n",
    "    return rows_to_save_db\r\n",
    "    # banco_olx.add_rows_anuncios_resumo(rows_to_save_db)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def scrape_and_insert_data_olx(url_end_point,urls_already_access=[],page_number=1,session=None,qt_try=1,force_search=False):\r\n",
    "    # print(f\"Scraping {url_end_point} page {page_number}\")\r\n",
    "    \r\n",
    "    query_Parameters = {'sf':1} if page_number == 1 else {} # get data in order of publication (THIS CASE IS POSSIBLE STOP SCRAPING IF SEE ONE AD ALREADY SCRAPED)\r\n",
    "    _session = session if session else requests.Session()\r\n",
    "    # try:\r\n",
    "    resp = _session.get(url_end_point, params=query_Parameters, headers=headers_olx, timeout=15)\r\n",
    "    bsObj = BeautifulSoup(resp.text, \"html.parser\")\r\n",
    "    _problem = True if bsObj.find('title',text=\"Ops!!\") else False\r\n",
    "    if not _problem :\r\n",
    "        _url_end_point = url_end_point.split('?')[0]\r\n",
    "        all_row_scraped = get_data_from_olx_page(bsObj,_url_end_point,page_number)\r\n",
    "        rows_to_save_db = [*filter(lambda x: x['url_anuncio'] not in urls_already_access, all_row_scraped)] # add only new ads\r\n",
    "        # banco_olx.add_rows_anuncios_resumo(rows_to_save_db)\r\n",
    "        \r\n",
    "        if len(rows_to_save_db) != len(all_row_scraped) and not force_search:  # if some ad was already scraped stop scraping in this page base\r\n",
    "            return len(rows_to_save_db)\r\n",
    "\r\n",
    "        next_page_tag = bsObj.find('a', {'data-lurker-detail':'next_page'})\r\n",
    "        next_page_url = next_page_tag['href'] if next_page_tag else None\r\n",
    "        if next_page_url:\r\n",
    "            return len(rows_to_save_db) + scrape_and_insert_data_olx(next_page_url,urls_already_access,page_number+1,_session,force_search=force_search)\r\n",
    "    else:\r\n",
    "        raise Exception(f\"Problem with Server in {url_end_point} page {page_number}\")\r\n",
    "    \r\n",
    "    # except Exception as e:\r\n",
    "    #         print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+ url_end_point)\r\n",
    "    #         print(str(e))\r\n",
    "    #         print(f'!!! Restart session and Wait {15*(qt_try**3)} seconds\\n\\n')\r\n",
    "    #         time.sleep(15*(qt_try**3))\r\n",
    "    #         _session = requests.Session()\r\n",
    "    #         if qt_try < 6:\r\n",
    "    #             return len(rows_to_save_db) + scrape_and_insert_data_olx(url_end_point,urls_already_access,page_number,_session,qt_try+1,force_search=force_search)\r\n",
    "    #         else:\r\n",
    "    #             print(f'!!! Error, qt attempts:{str(qt_try)} in:{url_end_point} page {page_number}')\r\n",
    "\r\n",
    "    return len(rows_to_save_db)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def scrape_all_ad_olx():\r\n",
    "    print('Scraping all ad data OLX')\r\n",
    "    start_urls_to_scraping  = banco_olx.get_urls_base_in_db(only_not_scraped=True)\r\n",
    "    if len(start_urls_to_scraping) == 0:\r\n",
    "        start_urls_to_scraping  = banco_olx.get_urls_base_in_db() # if all urls already scraped, update all data\r\n",
    "    rand_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=15))\r\n",
    "    for url in start_urls_to_scraping:\r\n",
    "        print(f\"Starting Scraping {url}\")\r\n",
    "        banco_olx.init_scraping_url_base(url,rand_string)\r\n",
    "        if banco_olx.verify_id_session(url,rand_string): # verify if session is the same (util for multiprocessing feature to use a cloud db)\r\n",
    "            current_reg_in_url = banco_olx.get_all_urls_ads(url)\r\n",
    "            qt_reg_add = scrape_and_insert_data_olx(url,current_reg_in_url)\r\n",
    "            banco_olx.end_scraping_url_base(url,qt_reg_add)\r\n",
    "            print(f\"End Scraping {url} - {qt_reg_add} new ads\")\r\n",
    "        else:\r\n",
    "            print(f\"End Scraping: Other process/computer is scraping {url}\")    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def scrape_data_olx_page(url_end_point,urls_already_access,force_search=False):\r\n",
    "    list_result_data=[]\r\n",
    "    session = requests.Session()\r\n",
    "\r\n",
    "    def scrape_data_olx_page_retroactive(url_end_point,page_number=1,qt_try=1):\r\n",
    "        # print(f\"Scraping {url_end_point} page {page_number}\")\r\n",
    "        nonlocal session\r\n",
    "        query_Parameters = {'sf':1} if page_number == 1 else {} # get data in order of publication (THIS CASE IS POSSIBLE STOP SCRAPING IF SEE ONE AD ALREADY SCRAPED)\r\n",
    "        try:\r\n",
    "            resp = session.get(url_end_point, params=query_Parameters, headers=headers_olx, timeout=15)\r\n",
    "            bsObj = BeautifulSoup(resp.text, \"html.parser\")\r\n",
    "            _problem = True if bsObj.find('title',text=\"Ops!!\") else False\r\n",
    "            if not _problem :\r\n",
    "                _url_end_point = url_end_point.split('?')[0]\r\n",
    "                all_row_scraped = get_data_from_olx_page(bsObj,_url_end_point,page_number)\r\n",
    "                rows_to_save_db = [*filter(lambda x: x['url_anuncio'] not in urls_already_access, all_row_scraped)] # add only new ads\r\n",
    "                list_result_data.extend(rows_to_save_db)\r\n",
    "\r\n",
    "                if len(rows_to_save_db) != len(all_row_scraped) and not force_search:  # if some ad was already scraped stop scraping in this page base\r\n",
    "                    return True\r\n",
    "\r\n",
    "                next_page_tag = bsObj.find('a', {'data-lurker-detail':'next_page'})\r\n",
    "                next_page_url = next_page_tag['href'] if next_page_tag else None\r\n",
    "                if next_page_url:\r\n",
    "                    return scrape_data_olx_page_retroactive(next_page_url,page_number+1,qt_try)\r\n",
    "            else:\r\n",
    "                raise Exception(f\"Problem with Server in {url_end_point} page {page_number}\")\r\n",
    "        \r\n",
    "        except Exception as e:\r\n",
    "                print('!!! Warning, qt attempts:'+str(qt_try)+' in:'+ url_end_point)\r\n",
    "                print(str(e))\r\n",
    "                print(f'!!! Restart session and Wait {15*(qt_try**3)} seconds\\n\\n')\r\n",
    "                time.sleep(15*(qt_try**3))\r\n",
    "                session = requests.Session()\r\n",
    "                if qt_try < 6:\r\n",
    "                    return scrape_data_olx_page_retroactive(url_end_point,page_number,qt_try+1)\r\n",
    "                else:\r\n",
    "                    print(f'!!! Error, qt attempts:{str(qt_try)} in:{url_end_point} page {page_number}')\r\n",
    "                    return False\r\n",
    "\r\n",
    "        return True\r\n",
    "    \r\n",
    "    scrape_data_olx_page_retroactive(url_end_point)\r\n",
    "    return list_result_data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# resultl = [[],[]]\r\n",
    "# erro = scrape_and_insert_data_olx('https://pi.olx.com.br/regiao-de-teresina-e-parnaiba/teresina/imoveis/venda/casas?o=93&sf=1')\r\n",
    "# jp2 = scrape_data_olx_page('https://ba.olx.com.br/grande-salvador/salvador/imoveis/venda/casas?o=100')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def scrape_all_ad_olx_threads(qt_threads=10,verbose=False):\r\n",
    "    print('Scraping all ad data OLX')\r\n",
    "    start_urls_to_scraping  = banco_olx.get_urls_base_in_db(only_not_scraped=False)\r\n",
    "    \r\n",
    "    if len(start_urls_to_scraping) == 0:\r\n",
    "        print('All urls already scraped, updating all data')\r\n",
    "        start_urls_to_scraping  = banco_olx.get_urls_base_in_db() # if all urls already scraped, update all data and scrape all data again in order of publication\r\n",
    "    else:\r\n",
    "        random.shuffle(start_urls_to_scraping) # randomize the order of scraping (AVOID COLLISIONS for first scraping base)\r\n",
    "\r\n",
    "    rand_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=15))\r\n",
    "    def _scrape_data_olx_page(url,results,index,current_reg_in_url):\r\n",
    "        #is necessay to use this function because the get_retroactive_links_in_div function is not thread safe\r\n",
    "        results[index] = scrape_data_olx_page(url,current_reg_in_url)\r\n",
    "    \r\n",
    "    threads = [None] * qt_threads\r\n",
    "    results = [None] * qt_threads\r\n",
    "    qt_urls_base = len(start_urls_to_scraping)\r\n",
    "\r\n",
    "    print('Start Scraping all ad data OLX')\r\n",
    "    for index in range(0,qt_urls_base,qt_threads):\r\n",
    "        for i in range(qt_threads):\r\n",
    "            if (index+i) < qt_urls_base:\r\n",
    "                print(f\"Starting Scraping {start_urls_to_scraping[index+i]}\")\r\n",
    "                banco_olx.init_scraping_url_base(start_urls_to_scraping[index+i],rand_string)\r\n",
    "                if banco_olx.verify_id_session(start_urls_to_scraping[index+i],rand_string): # verify if other process/computer is scraping this url\r\n",
    "                    current_reg_in_url = banco_olx.get_all_urls_ads(start_urls_to_scraping[index+i])\r\n",
    "                    threads[i] = Thread(target= _scrape_data_olx_page, args=(start_urls_to_scraping[index+i],results,i,current_reg_in_url))\r\n",
    "                    threads[i].start()\r\n",
    "                else:\r\n",
    "                    warning  = f\"End Scraping: Other process/computer is scraping {start_urls_to_scraping[index+i]}\"\r\n",
    "                    print(warning)\r\n",
    "                    threads[i] = Thread(target= print, args=(''))\r\n",
    "                    threads[i].start()\r\n",
    "\r\n",
    "        for i in range(qt_threads):\r\n",
    "            if (index+i) < qt_urls_base:\r\n",
    "                threads[i].join()\r\n",
    "                if results[i]!=None:\r\n",
    "                    qt_new_ads = len(results[i])\r\n",
    "                    if qt_new_ads>0 :\r\n",
    "                        banco_olx.add_rows_anuncios_resumo(results[i]) # need add rows in the same thread to avoid concurrency problems\r\n",
    "                    banco_olx.end_scraping_url_base(start_urls_to_scraping[index+i],qt_new_ads)\r\n",
    "                    print(f\"End Scraping {start_urls_to_scraping[index+i]} - {qt_new_ads} new ads\")\r\n",
    " \r\n",
    "        threads = [None] * qt_threads\r\n",
    "        results = [None] * qt_threads\r\n",
    "    \r\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scrape_all_ad_olx_threads(30)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# scrape_all_ad_olx()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# scrape_and_insert_data_olx('https://rj.olx.com.br/rio-de-janeiro-e-regiao/autos-e-pecas',urls_already_access=banco_olx.get_all_urls_ads('https://rj.olx.com.br/rio-de-janeiro-e-regiao/autos-e-pecas'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbf242f9ae95059c186c6323b314d6698994e3ef345fd3978f42b2a4e2555921"
   }
  },
  "interpreter": {
   "hash": "56a7faacf1c4dc0f5d7b7afc61346b24d10927f5de434ccac7dd24944d9b691b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}